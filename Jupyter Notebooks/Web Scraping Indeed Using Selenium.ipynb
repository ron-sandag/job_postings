{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2169589f",
   "metadata": {},
   "source": [
    "# In this notebook, we will scrape job posting data from Indeed for Employment data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71885820",
   "metadata": {},
   "source": [
    "The majority of this project is based on this article by Bonnie Ma:\n",
    "https://towardsdatascience.com/web-scraping-job-postings-from-indeed-com-using-selenium-5ae58d155daf\n",
    "\n",
    "First, we import selenium and specify the driver path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a11c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import os.path\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#specify driver path\n",
    "DRIVER_PATH = '/Users/rla/OneDrive - San Diego Association of Governments/Desktop/edgedriver_win64/msedgedriver'\n",
    "service = Service(DRIVER_PATH)\n",
    "driver = webdriver.Edge(service = service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8216e8",
   "metadata": {},
   "source": [
    "Next, we need navigate to our target website (Indeed.com) and perform a search with the correct settings before we can begin scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bbfb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on the \"Find Jobs\" button to get search results\n",
    "#initial_search_button = driver.find_element(By.XPATH, '//*[@id=\"jobsearch\"]/button')\n",
    "#initial_search_button.click()\n",
    "\n",
    "#assign the correct search settings\n",
    "\n",
    "#set the search radius to \"within 50 miles\" to cover San Diego County\n",
    "#filter_radius_button = driver.find_element(By.ID, \"filter-radius\")\n",
    "#filter_radius_button.click()\n",
    "#radius_menu = driver.find_element(By.ID, \"filter-radius-menu\")\n",
    "#fifty_mile_radius_button = radius_menu.find_element(By.XPATH, \".//li[6]\")\n",
    "#fifty_mile_radius_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e805b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigate to indeed.com\n",
    "# This webpage already has the search settings we are looking for\n",
    "driver.get('https://indeed.com/jobs?q=&l=San+Diego%2C+CA&radius=50&sort=date')\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bb7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_date(post_date, debug=False):\n",
    "    if debug:\n",
    "        print(post_date)\n",
    "    if post_date is None:\n",
    "        return None\n",
    "    x = re.search(r\"Today|Just posted|Posted\", post_date)\n",
    "    if x == None:\n",
    "        return datetime.datetime.now().date()\n",
    "    if (x):\n",
    "        days_past = 0\n",
    "    else:\n",
    "        x = re.search(r\"\\d+\", post_date)\n",
    "        days_past = int(x.group())\n",
    "\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    delta = datetime.timedelta(days=days_past)\n",
    "    final_date =(current_datetime-delta).date() \n",
    "    return final_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3fcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin scraping data\n",
    "\n",
    "def web_scraper():\n",
    "    \n",
    "    #close the pop-up that appears\n",
    "    try:\n",
    "        close_popup = driver.find_element(By.ID, \"popover-x\")\n",
    "    except:\n",
    "        print(\"No pop-up found. Continuing\")\n",
    "    else:\n",
    "        close_popup.click()\n",
    "\n",
    "    titles=[]\n",
    "    names=[]\n",
    "    locations=[]\n",
    "    remotes=[]\n",
    "    salaries=[]\n",
    "    full_times=[]\n",
    "    part_times=[]\n",
    "    post_dates=[]\n",
    "\n",
    "    #begin iterating through result pages\n",
    "\n",
    "    print(\"Begin scraping page\")\n",
    "    \n",
    "    try:\n",
    "        #let the driver wait 15 seconds to locate the element\n",
    "        # TODO: incorporate random wait time\n",
    "        driver.implicitly_wait(random.randrange(15,40))\n",
    "        job_card = driver.find_elements(By.XPATH, '//div[@class=\"job_seen_beacon\"]')\n",
    "    except:\n",
    "        driver.navigate().refresh()\n",
    "\n",
    "    for job in job_card:\n",
    "        #extract the job title\n",
    "        title = job.find_element(By.XPATH, './/td[@class=\"resultContent\"]/div/h2/span').text\n",
    "        titles.append(title)\n",
    "\n",
    "        #extract the company name\n",
    "        name = job.find_element(By.XPATH, './/span[@class=\"companyName\"]').text\n",
    "        names.append(name)    \n",
    "\n",
    "        #extract the company location\n",
    "        location = job.find_element(By.XPATH, './/div[@class=\"companyLocation\"]').text\n",
    "        locations.append(location)\n",
    "\n",
    "        #extract remote workplace information\n",
    "        x = re.search(r\"Remote\", location)\n",
    "        if (x):\n",
    "            remotes.append(True)\n",
    "        else:\n",
    "            remotes.append(False)\n",
    "\n",
    "        #extract metadata information\n",
    "        try:\n",
    "            metadata = job.find_element(By.XPATH, './/td[@class=\"resultContent\"]/div[3]').text\n",
    "            metadata = metadata.split(\"\\n\")\n",
    "        except:\n",
    "            metadata = None\n",
    "\n",
    "        #extract salary from metadata\n",
    "        salary=None\n",
    "        for data in metadata:\n",
    "            if '$' in data:\n",
    "                salary = data\n",
    "        salaries.append(salary)\n",
    "\n",
    "        #extract job type from metadata\n",
    "        full_time=False\n",
    "        part_time=False\n",
    "        for data in metadata:\n",
    "            if \"Full-time\" in data:\n",
    "                full_time = True\n",
    "            if \"Part-time\" in data:\n",
    "                part_time = True\n",
    "        full_times.append(full_time)\n",
    "        part_times.append(part_time)\n",
    "\n",
    "        #extract the job posting date\n",
    "        post_date = job.find_element(By.XPATH, '(.//span[@class=\"date\"])[last()]').text\n",
    "        \n",
    "        # Convert posted info into actual date\n",
    "        final_date = convert_text_to_date(post_date)\n",
    "        post_dates.append(final_date)\n",
    "\n",
    "    print(\"Finished scraping page\")\n",
    "    \n",
    "    #put all the data into a DataFrame\n",
    "    job_postings_df = pd.DataFrame()\n",
    "    job_postings_df['Job Title'] = titles\n",
    "    job_postings_df['Company Name'] = names\n",
    "    job_postings_df['Location'] = locations\n",
    "    job_postings_df['Remote'] = remotes\n",
    "    job_postings_df['Salary'] = salaries\n",
    "    job_postings_df['Full Time'] = full_times\n",
    "    job_postings_df['Part Time'] = part_times\n",
    "    job_postings_df['Date Posted'] = post_dates\n",
    "\n",
    "    # Check to see if cumulative job posting file exists\n",
    "    file_path = '../Data/Job_posting_data_current.csv'\n",
    "    flag = os.path.isfile(file_path)\n",
    "    \n",
    "    if flag:\n",
    "        #open the cumulative CSV file as a DataFrame\n",
    "        old_df = pd.read_csv(file_path)\n",
    "        #append the DataFrames together\n",
    "        combined_job_posting_df = old_df.append(job_postings_df)\n",
    "    else:\n",
    "        combined_job_posting_df = job_postings_df\n",
    "    \n",
    "    #save the new DataFrame as a CSV file\n",
    "    print(\"Saving data\")\n",
    "    combined_job_posting_df.to_csv(file_path, index=False)\n",
    "\n",
    "    #click for the next page of results\n",
    "    next_page_exists = True\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, '//a[@aria-label=\"Next\"]//span[@class=\"np\"]')\n",
    "        print(\"Continuing to next page\\n\")\n",
    "        next_page.click()\n",
    "    except:\n",
    "        print(\"Unable to find next page arrow\\n\")\n",
    "        next_page_exists = False\n",
    "    return next_page_exists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a5d527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "No pop-up found. Continuing\n",
      "Begin scraping page\n",
      "Finished scraping page\n",
      "Saving data\n",
      "Continuing to next page\n",
      "\n",
      "Scraping page 2\n",
      "Begin scraping page\n",
      "Finished scraping page\n",
      "Saving data\n",
      "Continuing to next page\n",
      "\n",
      "Scraping page 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'click'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1772/930555935.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mnext_page_exists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scraping page \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnext_page_exists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweb_scraper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mcurrent_page\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1772/420448826.py\u001b[0m in \u001b[0;36mweb_scraper\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No pop-up found. Continuing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mclose_popup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtitles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'click'"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "next_page_exists = True\n",
    "current_page = 1\n",
    "while next_page_exists:\n",
    "    print(\"Scraping page \" + str(current_page))\n",
    "    next_page_exists = web_scraper()\n",
    "    current_page += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52fbb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Scraping finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980474ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
